{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM Directional Classifier Training\n",
    "\n",
    "**Hypothesis 2**: A global directional classifier (LightGBM) with per-stock confidence thresholds can generate alpha.\n",
    "\n",
    "## Approach\n",
    "- Global model: one model across all stocks (captures cross-asset patterns)\n",
    "- Binary classification: UP/DOWN direction\n",
    "- Per-stock thresholds τ*: only trade when confidence > τ*\n",
    "- Walk-forward validation: train pre-2022, optimize thresholds 2022-2023, test 2024+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from utils.features import compute_features, forward_return_direction, multi_timeframe_returns\n",
    "from utils.evaluation import performance_report, print_report, sharpe_ratio\n",
    "from utils.data_loaders import load_yahoo\n",
    "from utils.model_store import ModelStore\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data — Diverse 19-Stock Universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\n",
    "    'AAPL', 'MSFT', 'NVDA', 'GOOGL',  # Tech\n",
    "    'JPM', 'BAC', 'GS',                 # Finance\n",
    "    'JNJ', 'UNH', 'PFE',                # Healthcare\n",
    "    'AMZN', 'WMT', 'KO',                # Consumer\n",
    "    'XOM', 'CVX',                        # Energy\n",
    "    'CAT', 'BA',                         # Industrial\n",
    "    'META', 'DIS',                       # Communication\n",
    "]\n",
    "\n",
    "df = load_yahoo(tickers, start='2018-01-01', end='2024-12-31')\n",
    "print(f'Loaded: {len(df)} rows, {df[\"ticker\"].nunique()} tickers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Features + Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_PERIODS = 5  # 5-day forward direction\n",
    "\n",
    "all_featured = []\n",
    "for ticker, group in df.groupby('ticker'):\n",
    "    group = group.sort_values('date').reset_index(drop=True)\n",
    "    featured = compute_features(group)\n",
    "    featured['ticker'] = ticker\n",
    "    featured['target'] = forward_return_direction(featured['close'], periods=TARGET_PERIODS)\n",
    "    all_featured.append(featured)\n",
    "\n",
    "data = pd.concat(all_featured, ignore_index=True).dropna().sort_values('date').reset_index(drop=True)\n",
    "\n",
    "exclude = ['date', 'ticker', 'target', 'open', 'high', 'low', 'close', 'volume', 'adj_close']\n",
    "feature_cols = [c for c in data.columns if c not in exclude]\n",
    "\n",
    "print(f'Feature matrix: {data.shape}')\n",
    "print(f'Features: {len(feature_cols)}')\n",
    "print(f'Target: {data[\"target\"].value_counts().to_dict()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Walk-Forward Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by date\n",
    "train_end = '2022-01-01'\n",
    "val_end = '2023-01-01'\n",
    "\n",
    "train = data[data['date'] < train_end]\n",
    "val = data[(data['date'] >= train_end) & (data['date'] < val_end)]\n",
    "test = data[data['date'] >= val_end]\n",
    "\n",
    "print(f'Train: {len(train)} rows ({train[\"date\"].min().date()} to {train[\"date\"].max().date()})')\n",
    "print(f'Val:   {len(val)} rows ({val[\"date\"].min().date()} to {val[\"date\"].max().date()})')\n",
    "print(f'Test:  {len(test)} rows ({test[\"date\"].min().date()} to {test[\"date\"].max().date()})')\n",
    "\n",
    "X_train, y_train = train[feature_cols].values, train['target'].values\n",
    "X_val, y_val = val[feature_cols].values, val['target'].values\n",
    "X_test, y_test = test[feature_cols].values, test['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(\n",
    "    num_leaves=31,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=500,\n",
    "    min_child_samples=20,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.1,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    verbose=-1,\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(50, verbose=False),\n",
    "        lgb.log_evaluation(period=50),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f'\\nBest iteration: {model.best_iteration_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)\n",
    "\n",
    "print('=== Test Set Results ===')\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_test, y_prob[:, 1]):.4f}')\n",
    "print(f'\\n{classification_report(y_test, y_pred, target_names=[\"DOWN\", \"UP\"])}')\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['DOWN', 'UP'], yticklabels=['DOWN', 'UP'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix — Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimize Per-Stock Thresholds\n",
    "\n",
    "Use validation set to find optimal confidence threshold τ* per stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation set with probabilities\n",
    "val_probs = model.predict_proba(X_val)[:, 1]  # P(UP)\n",
    "val['prob_up'] = val_probs\n",
    "\n",
    "# Find optimal threshold per stock (maximize accuracy on val set)\n",
    "optimal_thresholds = {}\n",
    "for ticker in tickers:\n",
    "    ticker_val = val[val['ticker'] == ticker]\n",
    "    if len(ticker_val) < 20:\n",
    "        optimal_thresholds[ticker] = 0.6  # default\n",
    "        continue\n",
    "    \n",
    "    best_threshold = 0.5\n",
    "    best_sharpe = -np.inf\n",
    "    \n",
    "    for threshold in np.arange(0.5, 0.85, 0.05):\n",
    "        # Simulate: buy when P(UP) > threshold\n",
    "        signals = ticker_val['prob_up'] > threshold\n",
    "        if signals.sum() < 5:\n",
    "            continue\n",
    "        # Simple return calculation\n",
    "        returns = ticker_val.loc[signals, 'close'].pct_change().dropna()\n",
    "        if len(returns) > 5:\n",
    "            sr = sharpe_ratio(returns)\n",
    "            if sr > best_sharpe:\n",
    "                best_sharpe = sr\n",
    "                best_threshold = threshold\n",
    "    \n",
    "    optimal_thresholds[ticker] = best_threshold\n",
    "\n",
    "print('Optimal thresholds per stock:')\n",
    "for ticker, t in sorted(optimal_thresholds.items()):\n",
    "    print(f'  {ticker}: {t:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "importance.head(20).plot(x='feature', y='importance', kind='barh', ax=ax)\n",
    "ax.set_title('Top 20 Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save locally\n",
    "store = ModelStore(local_dir='../../models')\n",
    "model_dir = store.save(\n",
    "    model,\n",
    "    'lightgbm_directional',\n",
    "    metadata={\n",
    "        'target_periods': TARGET_PERIODS,\n",
    "        'n_features': len(feature_cols),\n",
    "        'feature_names': feature_cols,\n",
    "        'optimal_thresholds': optimal_thresholds,\n",
    "        'test_accuracy': float(accuracy_score(y_test, y_pred)),\n",
    "        'test_auc': float(roc_auc_score(y_test, y_prob[:, 1])),\n",
    "    },\n",
    ")\n",
    "print(f'Model saved to: {model_dir}')\n",
    "\n",
    "# Save feature names for QC\n",
    "import json\n",
    "with open(f'{model_dir}/feature_names.json', 'w') as f:\n",
    "    json.dump(feature_cols, f)\n",
    "print('Feature names saved for QuantConnect integration.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "- [ ] Upload model to QC ObjectStore for live strategy use\n",
    "- [ ] Run backtest in QC with the directional_classifier strategy\n",
    "- [ ] Compare to SPY buy-and-hold\n",
    "- [ ] Test with different target periods (1, 5, 10, 21 days)\n",
    "- [ ] Try accumulative learning (transfer weights across walk-forward folds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
